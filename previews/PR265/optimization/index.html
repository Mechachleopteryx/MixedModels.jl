<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Details of the parameter estimation · MixedModels</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="MixedModels logo"/></a><div class="docs-package-name"><span class="docs-autofit">MixedModels</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">MixedModels.jl Documentation</a></li><li><a class="tocitem" href="../constructors/">Model constructors</a></li><li class="is-active"><a class="tocitem" href>Details of the parameter estimation</a><ul class="internal"><li><a class="tocitem" href="#The-probability-model-1"><span>The probability model</span></a></li><li><a class="tocitem" href="#Linear-Mixed-Effects-Models-1"><span>Linear Mixed-Effects Models</span></a></li><li><a class="tocitem" href="#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z-1"><span>Internal structure of <span>$\Lambda_\theta$</span> and <span>$\bf Z$</span></span></a></li><li><a class="tocitem" href="#Generalized-Linear-Mixed-Effects-Models-1"><span>Generalized Linear Mixed-Effects Models</span></a></li></ul></li><li><a class="tocitem" href="../GaussHermite/">Normalized Gauss-Hermite Quadrature</a></li><li><a class="tocitem" href="../bootstrap/">Parametric bootstrap for linear mixed-effects models</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Details of the parameter estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Details of the parameter estimation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaStats/MixedModels.jl/blob/master/docs/src/optimization.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Details-of-the-parameter-estimation-1"><a class="docs-heading-anchor" href="#Details-of-the-parameter-estimation-1">Details of the parameter estimation</a><a class="docs-heading-anchor-permalink" href="#Details-of-the-parameter-estimation-1" title="Permalink"></a></h1><h2 id="The-probability-model-1"><a class="docs-heading-anchor" href="#The-probability-model-1">The probability model</a><a class="docs-heading-anchor-permalink" href="#The-probability-model-1" title="Permalink"></a></h2><p>Maximum likelihood estimates are based on the probability model for the observed responses. In the probability model the distribution of the responses is expressed as a function of one or more <em>parameters</em>.</p><p>For a continuous distribution the probability density is a function of the responses, given the parameters. The <em>likelihood</em> function is the same expression as the probability density but regarding the observed values as fixed and the parameters as varying.</p><p>In general a mixed-effects model incorporates two random variables: <span>$\mathcal{B}$</span>, the <span>$q$</span>-dimensional vector of random effects, and <span>$\mathcal{Y}$</span>, the <span>$n$</span>-dimensional response vector. The value, <span>$\bf y$</span>, of <span>$\mathcal{Y}$</span> is observed; the value, <span>$\bf b$</span>, of <span>$\mathcal{B}$</span> is not.</p><h2 id="Linear-Mixed-Effects-Models-1"><a class="docs-heading-anchor" href="#Linear-Mixed-Effects-Models-1">Linear Mixed-Effects Models</a><a class="docs-heading-anchor-permalink" href="#Linear-Mixed-Effects-Models-1" title="Permalink"></a></h2><p>In a linear mixed model the unconditional distribution of <span>$\mathcal{B}$</span> and the conditional distribution, <span>$(\mathcal{Y} | \mathcal{B}=\bf{b})$</span>, are both multivariate Gaussian distributions,</p><div>\[\begin{aligned}
  (\mathcal{Y} | \mathcal{B}=\bf{b}) &amp;\sim\mathcal{N}(\bf{ X\beta + Z b},\sigma^2\bf{I})\\\\
  \mathcal{B}&amp;\sim\mathcal{N}(\bf{0},\Sigma_\theta) .
\end{aligned}\]</div><p>The <em>conditional mean</em> of <span>$\mathcal Y$</span>, given <span>$\mathcal B=\bf b$</span>, is the <em>linear predictor</em>, <span>$\bf X\bf\beta+\bf Z\bf b$</span>, which depends on the <span>$p$</span>-dimensional <em>fixed-effects parameter</em>, <span>$\bf \beta$</span>, and on <span>$\bf b$</span>. The <em>model matrices</em>, <span>$\bf X$</span> and <span>$\bf Z$</span>, of dimension <span>$n\times p$</span> and <span>$n\times q$</span>, respectively, are determined from the formula for the model and the values of covariates. Although the matrix <span>$\bf Z$</span> can be large (i.e. both <span>$n$</span> and <span>$q$</span> can be large), it is sparse (i.e. most of the elements in the matrix are zero).</p><p>The <em>relative covariance factor</em>, <span>$\Lambda_\theta$</span>, is a <span>$q\times q$</span> lower-triangular matrix, depending on the <em>variance-component parameter</em>, <span>$\bf\theta$</span>, and generating the symmetric <span>$q\times q$</span> variance-covariance matrix, <span>$\Sigma_\theta$</span>, as</p><div>\[\Sigma_\theta=\sigma^2\Lambda_\theta\Lambda_\theta&#39;\]</div><p>The <em>spherical random effects</em>, <span>$\mathcal{U}\sim\mathcal{N}(\bf{0},\sigma^2\bf{I}_q)$</span>, determine <span>$\mathcal B$</span> according to</p><div>\[\mathcal{B}=\Lambda_\theta\mathcal{U}.\]</div><p>The <em>penalized residual sum of squares</em> (PRSS),</p><div>\[r^2(\theta,\beta,\bf{u})=\|\bf{y} - \bf{X}\beta -\bf{Z}\Lambda_\theta\bf{u}\|^2+\|\bf{u}\|^2,\]</div><p>is the sum of the residual sum of squares, measuring fidelity of the model to the data, and a penalty on the size of <span>$\bf u$</span>, measuring the complexity of the model. Minimizing <span>$r^2$</span> with respect to <span>$\bf u$</span>,</p><div>\[r^2_{\beta,\theta} =\min_{\bf{u}}\left(\|\bf{y} -\bf{X}{\beta} -\bf{Z}\Lambda_\theta\bf{u}\|^2+\|\bf{u}\|^2\right)\]</div><p>is a direct (i.e. non-iterative) computation. The particular method used to solve this generates a <em>blocked Choleksy factor</em>, <span>$\bf{L}_\theta$</span>, which is an lower triangular <span>$q\times q$</span> matrix satisfying</p><div>\[\bf{L}_\theta\bf{L}_\theta&#39;=\Lambda_\theta&#39;\bf{Z}&#39;\bf{Z}\Lambda_\theta+\bf{I}_q .\]</div><p>where <span>${\bf I}_q$</span> is the <span>$q\times q$</span> <em>identity matrix</em>.</p><p>Negative twice the log-likelihood of the parameters, given the data, <span>$\bf y$</span>, is</p><div>\[d({\bf\theta},{\bf\beta},\sigma|{\bf y})
=n\log(2\pi\sigma^2)+\log(|{\bf L}_\theta|^2)+\frac{r^2_{\beta,\theta}}{\sigma^2}.\]</div><p>where <span>$|{\bf L}_\theta|$</span> denotes the <em>determinant</em> of <span>${\bf L}_\theta$</span>. Because <span>${\bf L}_\theta$</span> is triangular, its determinant is the product of its diagonal elements.</p><p>Because the conditional mean, <span>$\bf\mu_{\mathcal Y|\mathcal B=\bf b}=\bf X\bf\beta+\bf Z\Lambda_\theta\bf u$</span>, is a linear function of both <span>$\bf\beta$</span> and <span>$\bf u$</span>, minimization of the PRSS with respect to both <span>$\bf\beta$</span> and <span>$\bf u$</span> to produce</p><div>\[r^2_\theta =\min_{{\bf\beta},{\bf u}}\left(\|{\bf y} -{\bf X}{\bf\beta} -{\bf Z}\Lambda_\theta{\bf u}\|^2+\|{\bf u}\|^2\right)\]</div><p>is also a direct calculation. The values of <span>$\bf u$</span> and <span>$\bf\beta$</span> that provide this minimum are called, respectively, the <em>conditional mode</em>, <span>$\tilde{\bf u}_\theta$</span>, of the spherical random effects and the conditional estimate, <span>$\widehat{\bf\beta}_\theta$</span>, of the fixed effects. At the conditional estimate of the fixed effects the objective is</p><div>\[d({\bf\theta},\widehat{\beta}_\theta,\sigma|{\bf y})
=n\log(2\pi\sigma^2)+\log(|{\bf L}_\theta|^2)+\frac{r^2_\theta}{\sigma^2}.\]</div><p>Minimizing this expression with respect to <span>$\sigma^2$</span> produces the conditional estimate</p><div>\[\widehat{\sigma^2}_\theta=\frac{r^2_\theta}{n}\]</div><p>which provides the <em>profiled log-likelihood</em> on the deviance scale as</p><div>\[\tilde{d}(\theta|{\bf y})=d(\theta,\widehat{\beta}_\theta,\widehat{\sigma}_\theta|{\bf y})
=\log(|{\bf L}_\theta|^2)+n\left[1+\log\left(\frac{2\pi r^2_\theta}{n}\right)\right],\]</div><p>a function of <span>$\bf\theta$</span> alone.</p><p>The MLE of <span>$\bf\theta$</span>, written <span>$\widehat{\bf\theta}$</span>, is the value that minimizes this profiled objective. We determine this value by numerical optimization. In the process of evaluating <span>$\tilde{d}(\widehat{\theta}|{\bf y})$</span> we determine <span>$\widehat{\beta}=\widehat{\beta}_{\widehat\theta}$</span>, <span>$\tilde{\bf u}_{\widehat{\theta}}$</span> and <span>$r^2_{\widehat{\theta}}$</span>, from which we can evaluate <span>$\widehat{\sigma}=\sqrt{r^2_{\widehat{\theta}}/n}$</span>.</p><p>The elements of the conditional mode of <span>$\mathcal B$</span>, evaluated at the parameter estimates,</p><div>\[\tilde{\bf b}_{\widehat{\theta}}=\Lambda_{\widehat{\theta}}\tilde{\bf u}_{\widehat{\theta}}\]</div><p>are sometimes called the <em>best linear unbiased predictors</em> or BLUPs of the random effects. Although BLUPs an appealing acronym, I don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best”?) and prefer the term “conditional modes”, because these are the values of <span>$\bf b$</span> that maximize the density of the conditional distribution <span>$\mathcal{B} | \mathcal{Y} = {\bf y}$</span>. For a linear mixed model, where all the conditional and unconditional distributions are Gaussian, these values are also the <em>conditional means</em>.</p><h2 id="Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z-1"><a class="docs-heading-anchor" href="#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z-1">Internal structure of <span>$\Lambda_\theta$</span> and <span>$\bf Z$</span></a><a class="docs-heading-anchor-permalink" href="#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z-1" title="Permalink"></a></h2><p>In the types of <code>LinearMixedModel</code> available through the <code>MixedModels</code> package, groups of random effects and the corresponding columns of the model matrix, <span>$\bf Z$</span>, are associated with <em>random-effects terms</em> in the model formula.</p><p>For the simple example</p><pre><code class="language-julia">julia&gt; dyestuff = MixedModels.dataset(:dyestuff)
30×2 DataFrames.DataFrame
│ Row │ batch  │ yield │
│     │ String │ Int16 │
├─────┼────────┼───────┤
│ 1   │ A      │ 1545  │
│ 2   │ A      │ 1440  │
│ 3   │ A      │ 1440  │
│ 4   │ A      │ 1520  │
│ 5   │ A      │ 1580  │
│ 6   │ B      │ 1540  │
│ 7   │ B      │ 1555  │
⋮
│ 23  │ E      │ 1515  │
│ 24  │ E      │ 1635  │
│ 25  │ E      │ 1625  │
│ 26  │ F      │ 1520  │
│ 27  │ F      │ 1455  │
│ 28  │ F      │ 1450  │
│ 29  │ F      │ 1480  │
│ 30  │ F      │ 1445  │

julia&gt; fm1 = fit(MixedModel, @formula(yield ~ 1 + (1|batch)), dyestuff)
Linear mixed model fit by maximum likelihood
 yield ~ 1 + (1 | batch)
   logLik   -2 logLik     AIC        BIC    
 -163.66353  327.32706  333.32706  337.53065

Variance components:
            Column    Variance  Std.Dev. 
batch    (Intercept)  1388.3333 37.260345
Residual              2451.2500 49.510100
 Number of obs: 30; levels of grouping factors: 6

  Fixed-effects parameters:
──────────────────────────────────────────────────
             Estimate  Std.Error  z value  P(&gt;|z|)
──────────────────────────────────────────────────
(Intercept)    1527.5    17.6946    86.33   &lt;1e-99
──────────────────────────────────────────────────
</code></pre><p>the only random effects term in the formula is <code>(1|batch)</code>, a simple, scalar random-effects term.</p><pre><code class="language-julia">julia&gt; t1 = first(fm1.reterms);

julia&gt; Int.(t1)  # convert to integers for more compact display
30×6 Array{Int64,2}:
 1  0  0  0  0  0
 1  0  0  0  0  0
 1  0  0  0  0  0
 1  0  0  0  0  0
 1  0  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 ⋮              ⋮
 0  0  0  0  1  0
 0  0  0  0  1  0
 0  0  0  0  1  0
 0  0  0  0  1  0
 0  0  0  0  0  1
 0  0  0  0  0  1
 0  0  0  0  0  1
 0  0  0  0  0  1
 0  0  0  0  0  1
</code></pre><article class="docstring"><header><a class="docstring-binding" id="MixedModels.ReMat" href="#MixedModels.ReMat"><code>MixedModels.ReMat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ReMat{T,S} &lt;: AbstractMatrix{T}</code></pre><p>A section of a model matrix generated by a random-effects term.</p><p><strong>Fields</strong></p><ul><li><code>trm</code>: the grouping factor as a <code>StatsModels.CategoricalTerm</code></li><li><code>refs</code>: indices into the levels of the grouping factor as a <code>Vector{Int32}</code></li><li><code>z</code>: transpose of the model matrix generated by the left-hand side of the term</li><li><code>wtz</code>: a weighted copy of <code>z</code> (<code>z</code> and <code>wtz</code> are the same object for unweighted cases)</li><li><code>λ</code>: a <code>LowerTriangular</code> matrix of size <code>S×S</code></li><li><code>inds</code>: a <code>Vector{Int}</code> of linear indices of the potential nonzeros in <code>λ</code></li><li><code>adjA</code>: the adjoint of the matrix as a <code>SparseMatrixCSC{T}</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/d94a23647c187c730794aed6d724507a7d027503/src/remat.jl#L1-L14">source</a></section></article><p>This <code>RandomEffectsTerm</code> contributes a block of columns to the model matrix <span>$\bf Z$</span> and a diagonal block to <span>$\Lambda_\theta$</span>. In this case the diagonal block of <span>$\Lambda_\theta$</span> (which is also the only block) is a multiple of the <span>$6\times6$</span> identity matrix where the multiple is</p><pre><code class="language-julia">julia&gt; t1.λ
1×1 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:
 0.7525806757718846
</code></pre><p>Because there is only one random-effects term in the model, the matrix <span>$\bf Z$</span> is the indicators matrix shown as the result of <code>Matrix(t1)</code>, but stored in a special sparse format. Furthermore, there is only one block in <span>$\Lambda_\theta$</span>.</p><p>For a vector-valued random-effects term, as in</p><pre><code class="language-julia">julia&gt; sleepstudy = MixedModels.dataset(:sleepstudy)
180×3 DataFrames.DataFrame
│ Row │ subj   │ days │ reaction │
│     │ String │ Int8 │ Float64  │
├─────┼────────┼──────┼──────────┤
│ 1   │ S308   │ 0    │ 249.56   │
│ 2   │ S308   │ 1    │ 258.705  │
│ 3   │ S308   │ 2    │ 250.801  │
│ 4   │ S308   │ 3    │ 321.44   │
│ 5   │ S308   │ 4    │ 356.852  │
│ 6   │ S308   │ 5    │ 414.69   │
│ 7   │ S308   │ 6    │ 382.204  │
⋮
│ 173 │ S372   │ 2    │ 297.597  │
│ 174 │ S372   │ 3    │ 310.632  │
│ 175 │ S372   │ 4    │ 287.173  │
│ 176 │ S372   │ 5    │ 329.608  │
│ 177 │ S372   │ 6    │ 334.482  │
│ 178 │ S372   │ 7    │ 343.22   │
│ 179 │ S372   │ 8    │ 369.142  │
│ 180 │ S372   │ 9    │ 364.124  │

julia&gt; fm2 = fit(MixedModel, @formula(reaction ~ 1+days+(1+days|subj)), sleepstudy)
Linear mixed model fit by maximum likelihood
 reaction ~ 1 + days + (1 + days | subj)
   logLik   -2 logLik     AIC        BIC    
 -875.96967 1751.93934 1763.93934 1783.09709

Variance components:
            Column    Variance  Std.Dev.   Corr.
subj     (Intercept)  565.51068 23.780469
         days          32.68212  5.716828  0.08
Residual              654.94145 25.591824
 Number of obs: 180; levels of grouping factors: 18

  Fixed-effects parameters:
──────────────────────────────────────────────────
             Estimate  Std.Error  z value  P(&gt;|z|)
──────────────────────────────────────────────────
(Intercept)  251.405     6.63226    37.91   &lt;1e-99
days          10.4673    1.50224     6.97   &lt;1e-11
──────────────────────────────────────────────────
</code></pre><p>the model matrix <span>$\bf Z$</span> is of the form</p><pre><code class="language-julia">julia&gt; t21 = first(fm2.reterms);

julia&gt; Int.(t21) # convert to integers for more compact display
180×36 Array{Int64,2}:
 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0
 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0
 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4
 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9
</code></pre><p>and <span>$\Lambda_\theta$</span> is a <span>$36\times36$</span> block diagonal matrix with <span>$18$</span> diagonal blocks, all of the form</p><pre><code class="language-julia">julia&gt; t21.λ
2×2 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:
 0.929221    ⋅      
 0.0181684  0.222645
</code></pre><p>The <span>$\theta$</span> vector is</p><pre><code class="language-julia">julia&gt; MixedModels.getθ(t21)
3-element Array{Float64,1}:
 0.9292213238823973  
 0.018168399088001212
 0.22264486437568012 
</code></pre><p>Random-effects terms in the model formula that have the same grouping factor are amagamated into a single <code>ReMat</code> object.</p><pre><code class="language-julia">julia&gt; fm3 = fit!(zerocorr!(LinearMixedModel(@formula(reaction ~ 1+days+(1+days|subj)),
    sleepstudy)))
Linear mixed model fit by maximum likelihood
 reaction ~ 1 + days + (1 + days | subj)
   logLik   -2 logLik     AIC        BIC    
 -876.00163 1752.00326 1762.00326 1777.96804

Variance components:
            Column    Variance  Std.Dev.   Corr.
subj     (Intercept)  584.258971 24.17145
         days          33.632805  5.79938   .  
Residual              653.115782 25.55613
 Number of obs: 180; levels of grouping factors: 18

  Fixed-effects parameters:
──────────────────────────────────────────────────
             Estimate  Std.Error  z value  P(&gt;|z|)
──────────────────────────────────────────────────
(Intercept)  251.405     6.70771    37.48   &lt;1e-99
days          10.4673    1.51931     6.89   &lt;1e-11
──────────────────────────────────────────────────

julia&gt; t31 = first(fm3.reterms);

julia&gt; Int.(t31)
180×36 Array{Int64,2}:
 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0
 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0
 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4
 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9
</code></pre><p>Note that we could also have achieved this by re-fitting (a copy of) <code>fm2</code>.</p><pre><code class="language-julia">julia&gt; fm3alt = fit!(zerocorr!(deepcopy(fm2)))
Linear mixed model fit by maximum likelihood
 reaction ~ 1 + days + (1 + days | subj)
   logLik   -2 logLik     AIC        BIC    
 -875.96967 1751.93934 1761.93934 1777.90413

Variance components:
            Column    Variance   Std.Dev.   Corr.
subj     (Intercept)  565.512842 23.780514
         days          32.682086  5.716825   .  
Residual              654.941299 25.591821
 Number of obs: 180; levels of grouping factors: 18

  Fixed-effects parameters:
──────────────────────────────────────────────────
             Estimate  Std.Error  z value  P(&gt;|z|)
──────────────────────────────────────────────────
(Intercept)  251.405     6.63227    37.91   &lt;1e-99
days          10.4673    1.50223     6.97   &lt;1e-11
──────────────────────────────────────────────────
</code></pre><p>For this model the matrix <span>$\bf Z$</span> is the same as that of model <code>fm2</code> but the diagonal blocks of <span>$\Lambda_\theta$</span> are themselves diagonal.</p><pre><code class="language-julia">julia&gt; t31.λ
2×2 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:
 0.945818   ⋅      
 0.0       0.226927

julia&gt; MixedModels.getθ(t31)
2-element Array{Float64,1}:
 0.945818066476781  
 0.22692714872124642
</code></pre><p>Random-effects terms with distinct grouping factors generate distinct elements of the <code>trms</code> member of the <code>LinearMixedModel</code> object. Multiple <code>ReMat</code> objects are sorted by decreasing numbers of random effects.</p><pre><code class="language-julia">julia&gt; penicillin = MixedModels.dataset(:penicillin)
144×3 DataFrames.DataFrame
│ Row │ plate  │ sample │ diameter │
│     │ String │ String │ Int8     │
├─────┼────────┼────────┼──────────┤
│ 1   │ a      │ A      │ 27       │
│ 2   │ a      │ B      │ 23       │
│ 3   │ a      │ C      │ 26       │
│ 4   │ a      │ D      │ 23       │
│ 5   │ a      │ E      │ 23       │
│ 6   │ a      │ F      │ 21       │
│ 7   │ b      │ A      │ 27       │
⋮
│ 137 │ w      │ E      │ 24       │
│ 138 │ w      │ F      │ 19       │
│ 139 │ x      │ A      │ 24       │
│ 140 │ x      │ B      │ 21       │
│ 141 │ x      │ C      │ 24       │
│ 142 │ x      │ D      │ 22       │
│ 143 │ x      │ E      │ 21       │
│ 144 │ x      │ F      │ 18       │

julia&gt; fm4 = fit(MixedModel,
    @formula(diameter ~ 1 + (1|plate) + (1|sample)),
    penicillin)
Linear mixed model fit by maximum likelihood
 diameter ~ 1 + (1 | plate) + (1 | sample)
   logLik   -2 logLik     AIC        BIC    
 -166.09417  332.18835  340.18835  352.06760

Variance components:
            Column    Variance   Std.Dev. 
plate    (Intercept)  0.71497945 0.8455646
sample   (Intercept)  3.13519425 1.7706480
Residual              0.30242640 0.5499331
 Number of obs: 144; levels of grouping factors: 24, 6

  Fixed-effects parameters:
──────────────────────────────────────────────────
             Estimate  Std.Error  z value  P(&gt;|z|)
──────────────────────────────────────────────────
(Intercept)   22.9722   0.744596    30.85   &lt;1e-99
──────────────────────────────────────────────────

julia&gt; Int.(first(fm4.reterms))
144×24 Array{Int64,2}:
 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 ⋮              ⋮              ⋮              ⋮              ⋮         
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1

julia&gt; Int.(last(fm4.reterms))
144×6 Array{Int64,2}:
 1  0  0  0  0  0
 0  1  0  0  0  0
 0  0  1  0  0  0
 0  0  0  1  0  0
 0  0  0  0  1  0
 0  0  0  0  0  1
 1  0  0  0  0  0
 0  1  0  0  0  0
 0  0  1  0  0  0
 0  0  0  1  0  0
 ⋮              ⋮
 0  0  0  1  0  0
 0  0  0  0  1  0
 0  0  0  0  0  1
 1  0  0  0  0  0
 0  1  0  0  0  0
 0  0  1  0  0  0
 0  0  0  1  0  0
 0  0  0  0  1  0
 0  0  0  0  0  1
</code></pre><p>Note that the first <code>ReMat</code> in <code>fm4.terms</code> corresponds to grouping factor <code>G</code> even though the term <code>(1|G)</code> occurs in the formula after <code>(1|H)</code>.</p><h3 id="Progress-of-the-optimization-1"><a class="docs-heading-anchor" href="#Progress-of-the-optimization-1">Progress of the optimization</a><a class="docs-heading-anchor-permalink" href="#Progress-of-the-optimization-1" title="Permalink"></a></h3><p>An optional named argument, <code>verbose=true</code>, in the call to <code>fit</code> for a <code>LinearMixedModel</code> causes printing of the objective and the <span>$\theta$</span> parameter at each evaluation during the optimization.</p><pre><code class="language-julia">julia&gt; fit(MixedModel,
    @formula(yield ~ 1 + (1|batch)),
    dyestuff,
    verbose=true);
f_1: 327.76702 [1.0]
f_2: 331.03619 [1.75]
f_3: 330.64583 [0.25]
f_4: 327.69511 [0.9761896354666064]
f_5: 327.56631 [0.9285689063998191]
f_6: 327.3826 [0.8333274482662446]
f_7: 327.35315 [0.8071883308443906]
f_8: 327.34663 [0.7996883308443905]
f_9: 327.341 [0.7921883308443906]
f_10: 327.33253 [0.7771883308443905]
f_11: 327.32733 [0.7471883308443905]
f_12: 327.32862 [0.7396883308443906]
f_13: 327.32706 [0.7527765100479509]
f_14: 327.32707 [0.7535265100479508]
f_15: 327.32706 [0.7525837539403791]
f_16: 327.32706 [0.7525087539403791]
f_17: 327.32706 [0.7525912539403792]
f_18: 327.32706 [0.7525806757718846]

julia&gt; fit(MixedModel,
    @formula(reaction ~ 1 + days + (1+days|subj)),
    sleepstudy,
    verbose=true);
f_1: 1784.6423 [1.0, 0.0, 1.0]
f_2: 1790.12564 [1.75, 0.0, 1.0]
f_3: 1798.99962 [1.0, 1.0, 1.0]
f_4: 1803.8532 [1.0, 0.0, 1.75]
f_5: 1800.61398 [0.25, 0.0, 1.0]
f_6: 1798.60463 [1.0, -1.0, 1.0]
f_7: 1752.26074 [1.0, 0.0, 0.25]
f_8: 1797.58769 [1.1832612965367522, -0.008661887957963445, 0.0]
f_9: 1754.95411 [1.075, 0.0, 0.32499999999999996]
f_10: 1753.69568 [0.8166315695342754, 0.01116725445691172, 0.28823768689692686]
f_11: 1754.817 [1.0, -0.07071067811865475, 0.19696699141100893]
f_12: 1753.10673 [0.9436827046397082, 0.06383542916431532, 0.26269630296458246]
f_13: 1752.93938 [0.9801419885634294, -0.026656844944242912, 0.27474275609538057]
f_14: 1752.25688 [0.9843428851817972, -0.013234749183472554, 0.24719098754487245]
f_15: 1752.05745 [0.9731403970871993, 0.0025378492297931723, 0.23791031400556598]
f_16: 1752.02239 [0.9545259030380779, 0.0038642106174747192, 0.23589201227597048]
f_17: 1752.02273 [0.9359285300960238, 0.0013317973252317912, 0.23444534669778083]
f_18: 1751.97169 [0.9549646039745407, 0.00790664248316571, 0.22904616789180773]
f_19: 1751.9526 [0.9533132639133618, 0.016627370638374613, 0.22576831302901973]
f_20: 1751.94852 [0.9469287318329175, 0.013076079968896705, 0.2228711267534202]
f_21: 1751.98718 [0.9334175303159152, 0.006137673807764107, 0.21895094167679682]
f_22: 1751.98321 [0.9515444328078329, 0.005788999136907504, 0.22061819881550318]
f_23: 1751.95197 [0.9528093408783747, 0.019033192067265246, 0.2241776090317882]
f_24: 1751.94628 [0.9463215304146443, 0.015373858743888129, 0.22508817725554886]
f_25: 1751.9467 [0.9471235457936569, 0.014889405831518926, 0.2248923477396395]
f_26: 1751.94757 [0.9464970169219107, 0.015464270390126697, 0.22581419823408094]
f_27: 1751.94531 [0.9460858412916352, 0.015793369920850595, 0.22444946255259282]
f_28: 1751.94418 [0.9453036925450446, 0.016690245692505455, 0.22336052907025575]
f_29: 1751.94353 [0.9440720727046517, 0.017210606426826435, 0.22271587718150054]
f_30: 1751.94244 [0.9412710977030493, 0.01630994633071143, 0.22252263170344286]
f_31: 1751.94217 [0.9390004002869106, 0.015899017065370346, 0.222131976946372]
f_32: 1751.94237 [0.9389790833803197, 0.016547964762318432, 0.2215617504885323]
f_33: 1751.94228 [0.938862818953401, 0.015246587961002082, 0.22268346178301046]
f_34: 1751.9422 [0.9382687962294433, 0.015732967024600662, 0.22202359841263414]
f_35: 1751.94131 [0.9388391785491706, 0.016637330597083028, 0.2226114401214759]
f_36: 1751.94093 [0.9383965534721752, 0.01739653520554578, 0.22281726224155726]
f_37: 1751.94057 [0.9370059169105476, 0.01804448872830839, 0.2225344763965165]
f_38: 1751.94018 [0.9341094753480741, 0.018735420237090173, 0.22194958627572592]
f_39: 1751.94008 [0.9326416012346034, 0.018924172940390394, 0.22172575640995892]
f_40: 1751.94027 [0.9313571538969319, 0.019008176238413707, 0.22130945884044123]
f_41: 1751.9415 [0.9328207195517433, 0.020645417460674914, 0.22136730759659157]
f_42: 1751.93949 [0.931867483429435, 0.017957376299442794, 0.22256364256481267]
f_43: 1751.93939 [0.9291674452199865, 0.01778242169437659, 0.22253384648762076]
f_44: 1751.9394 [0.9296587607348034, 0.017772086223997605, 0.22250844015426333]
f_45: 1751.93943 [0.9291934578164848, 0.018780629224672163, 0.22257042607151467]
f_46: 1751.93935 [0.9289855868823702, 0.01823660315354116, 0.22248439976183174]
f_47: 1751.93949 [0.9286970922171983, 0.01829369871579516, 0.22317536801913518]
f_48: 1751.93936 [0.9282426017326223, 0.018269524695975977, 0.2225837144839457]
f_49: 1751.93934 [0.929112736142292, 0.01817912528230166, 0.2226238886269671]
f_50: 1751.93934 [0.9291905819450385, 0.018165752406344854, 0.22264320493118056]
f_51: 1751.93935 [0.9292542879999808, 0.018209268574409104, 0.2226208139381923]
f_52: 1751.93935 [0.9291892156985542, 0.018129784953348823, 0.2225732374320946]
f_53: 1751.93934 [0.9292536020579557, 0.018167625419705388, 0.22264990633508935]
f_54: 1751.93934 [0.9292145090654311, 0.018171739835744945, 0.22264674332079784]
f_55: 1751.93934 [0.9292083907701464, 0.018171479749592906, 0.22264619531962188]
f_56: 1751.93934 [0.9292093025431701, 0.018172967050564252, 0.22265206262238035]
f_57: 1751.93934 [0.9292213238823973, 0.018168399088001212, 0.22264486437568012]
</code></pre><p>A shorter summary of the optimization process is always available as an</p><article class="docstring"><header><a class="docstring-binding" id="MixedModels.OptSummary" href="#MixedModels.OptSummary"><code>MixedModels.OptSummary</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">OptSummary</code></pre><p>Summary of an <code>NLopt</code> optimization</p><p><strong>Fields</strong></p><ul><li><code>initial</code>: a copy of the initial parameter values in the optimization</li><li><code>lowerbd</code>: lower bounds on the parameter values</li><li><code>ftol_rel</code>: as in NLopt</li><li><code>ftol_abs</code>: as in NLopt</li><li><code>xtol_rel</code>: as in NLopt</li><li><code>xtol_abs</code>: as in NLopt</li><li><code>initial_step</code>: as in NLopt</li><li><code>maxfeval</code>: as in NLopt</li><li><code>final</code>: a copy of the final parameter values from the optimization</li><li><code>fmin</code>: the final value of the objective</li><li><code>feval</code>: the number of function evaluations</li><li><code>optimizer</code>: the name of the optimizer used, as a <code>Symbol</code></li><li><code>returnvalue</code>: the return value, as a <code>Symbol</code></li><li><code>nAGQ</code>: number of adaptive Gauss-Hermite quadrature points in deviance evaluation for GLMMs</li><li><code>REML</code>: use the REML criterion for LMM fits</li></ul><p>The latter field doesn&#39;t really belong here but it has to be in a mutable struct in case it is changed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/d94a23647c187c730794aed6d724507a7d027503/src/optsummary.jl#L1-L25">source</a></section></article><p>object, which is the <code>optsum</code> member of the <code>LinearMixedModel</code>.</p><pre><code class="language-julia">julia&gt; fm2.optsum
Initial parameter vector: [1.0, 0.0, 1.0]
Initial objective value:  1784.6422961924677

Optimizer (from NLopt):   LN_BOBYQA
Lower bounds:             [0.0, -Inf, 0.0]
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]
initial_step:             [0.75, 1.0, 0.75]
maxfeval:                 -1

Function evaluations:     57
Final parameter vector:   [0.9292213238823973, 0.018168399088001212, 0.22264486437568012]
Final objective value:    1751.9393444647196
Return code:              FTOL_REACHED

</code></pre><h3 id="Modifying-the-optimization-process-1"><a class="docs-heading-anchor" href="#Modifying-the-optimization-process-1">Modifying the optimization process</a><a class="docs-heading-anchor-permalink" href="#Modifying-the-optimization-process-1" title="Permalink"></a></h3><p>The <code>OptSummary</code> object contains both input and output fields for the optimizer. To modify the optimization process the input fields can be changed after constructing the model but before fitting it.</p><p>Suppose, for example, that the user wishes to try a <a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">Nelder-Mead</a> optimization method instead of the default <a href="https://en.wikipedia.org/wiki/BOBYQA"><code>BOBYQA</code></a> (Bounded Optimization BY Quadratic Approximation) method.</p><pre><code class="language-julia">julia&gt; fm2 = LinearMixedModel(@formula(reaction ~ 1+days+(1+days|subj)), sleepstudy);

julia&gt; fm2.optsum.optimizer = :LN_NELDERMEAD;

julia&gt; fit!(fm2)
Linear mixed model fit by maximum likelihood
 reaction ~ 1 + days + (1 + days | subj)
   logLik   -2 logLik     AIC        BIC    
 -875.96967 1751.93934 1763.93934 1783.09709

Variance components:
            Column    Variance   Std.Dev.   Corr.
subj     (Intercept)  565.528831 23.780850
         days          32.681047  5.716734  0.08
Residual              654.941678 25.591828
 Number of obs: 180; levels of grouping factors: 18

  Fixed-effects parameters:
──────────────────────────────────────────────────
             Estimate  Std.Error  z value  P(&gt;|z|)
──────────────────────────────────────────────────
(Intercept)  251.405     6.63233    37.91   &lt;1e-99
days          10.4673    1.50222     6.97   &lt;1e-11
──────────────────────────────────────────────────

julia&gt; fm2.optsum
Initial parameter vector: [1.0, 0.0, 1.0]
Initial objective value:  1784.6422961924677

Optimizer (from NLopt):   LN_NELDERMEAD
Lower bounds:             [0.0, -Inf, 0.0]
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]
initial_step:             [0.75, 1.0, 0.75]
maxfeval:                 -1

Function evaluations:     140
Final parameter vector:   [0.9292360739538559, 0.018168794976407835, 0.22264111430139058]
Final objective value:    1751.9393444750297
Return code:              FTOL_REACHED

</code></pre><p>The parameter estimates are quite similar to those using <code>:LN_BOBYQA</code> but at the expense of 140 functions evaluations for <code>:LN_NELDERMEAD</code> versus 57 for <code>:LN_BOBYQA</code>.</p><p>See the documentation for the <a href="https://github.com/JuliaOpt/NLopt.jl"><code>NLopt</code></a> package for details about the various settings.</p><h3 id="Convergence-to-singular-covariance-matrices-1"><a class="docs-heading-anchor" href="#Convergence-to-singular-covariance-matrices-1">Convergence to singular covariance matrices</a><a class="docs-heading-anchor-permalink" href="#Convergence-to-singular-covariance-matrices-1" title="Permalink"></a></h3><p>To ensure identifiability of <span>$\Sigma_\theta=\sigma^2\Lambda_\theta \Lambda_\theta$</span>, the elements of <span>$\theta$</span> corresponding to diagonal elements of <span>$\Lambda_\theta$</span> are constrained to be non-negative. For example, in a trivial case of a single, simple, scalar, random-effects term as in <code>fm1</code>, the one-dimensional <span>$\theta$</span> vector is the ratio of the standard deviation of the random effects to the standard deviation of the response. It happens that <span>$-\theta$</span> produces the same log-likelihood but, by convention, we define the standard deviation to be the positive square root of the variance. Requiring the diagonal elements of <span>$\Lambda_\theta$</span> to be non-negative is a generalization of using this positive square root.</p><p>If the optimization converges on the boundary of the feasible region, that is if one or more of the diagonal elements of <span>$\Lambda_\theta$</span> is zero at convergence, the covariance matrix <span>$\Sigma_\theta$</span> will be <em>singular</em>. This means that there will be linear combinations of random effects that are constant. Usually convergence to a singular covariance matrix is a sign of an over-specified model.</p><p>Singularity can be checked with the <code>issingular</code> predicate function.</p><article class="docstring"><header><a class="docstring-binding" id="MixedModels.issingular" href="#MixedModels.issingular"><code>MixedModels.issingular</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">issingular(m::LinearMixedModel, θ=m.θ)</code></pre><p>Test whether the model <code>m</code> is singular if the parameter vector is <code>θ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/d94a23647c187c730794aed6d724507a7d027503/src/linearmixedmodel.jl#L474-L478">source</a></section></article><pre><code class="language-julia">julia&gt; issingular(fm2)
false
</code></pre><h2 id="Generalized-Linear-Mixed-Effects-Models-1"><a class="docs-heading-anchor" href="#Generalized-Linear-Mixed-Effects-Models-1">Generalized Linear Mixed-Effects Models</a><a class="docs-heading-anchor-permalink" href="#Generalized-Linear-Mixed-Effects-Models-1" title="Permalink"></a></h2><p>In a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model"><em>generalized linear model</em></a> the responses are modelled as coming from a particular distribution, such as <code>Bernoulli</code> for binary responses or <code>Poisson</code> for responses that represent counts. The scalar distributions of individual responses differ only in their means, which are determined by a <em>linear predictor</em> expression <span>$\eta=\bf X\beta$</span>, where, as before, <span>$\bf X$</span> is a model matrix derived from the values of covariates and <span>$\beta$</span> is a vector of coefficients.</p><p>The unconstrained components of <span>$\eta$</span> are mapped to the, possiby constrained, components of the mean response, <span>$\mu$</span>, via a scalar function, <span>$g^{-1}$</span>, applied to each component of <span>$\eta$</span>. For historical reasons, the inverse of this function, taking components of <span>$\mu$</span> to the corresponding component of <span>$\eta$</span> is called the <em>link function</em> and more frequently used map from <span>$\eta$</span> to <span>$\mu$</span> is the <em>inverse link</em>.</p><p>A <em>generalized linear mixed-effects model</em> (GLMM) is defined, for the purposes of this package, by</p><div>\[\begin{aligned}
  (\mathcal{Y} | \mathcal{B}=\bf{b}) &amp;\sim\mathcal{D}(\bf{g^{-1}(X\beta + Z b)},\phi)\\\\
  \mathcal{B}&amp;\sim\mathcal{N}(\bf{0},\Sigma_\theta) .
\end{aligned}\]</div><p>where <span>$\mathcal{D}$</span> indicates the distribution family parameterized by the mean and, when needed, a common scale parameter, <span>$\phi$</span>. (There is no scale parameter for <code>Bernoulli</code> or for <code>Poisson</code>. Specifying the mean completely determines the distribution.)</p><article class="docstring"><header><a class="docstring-binding" id="Distributions.Bernoulli" href="#Distributions.Bernoulli"><code>Distributions.Bernoulli</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Bernoulli(p)</code></pre><p>A <em>Bernoulli distribution</em> is parameterized by a success rate <code>p</code>, which takes value 1 with probability <code>p</code> and 0 with probability <code>1-p</code>.</p><div>\[P(X = k) = \begin{cases}
1 - p &amp; \quad \text{for } k = 0, \\
p &amp; \quad \text{for } k = 1.
\end{cases}\]</div><pre><code class="language-julia">Bernoulli()    # Bernoulli distribution with p = 0.5
Bernoulli(p)   # Bernoulli distribution with success rate p

params(d)      # Get the parameters, i.e. (p,)
succprob(d)    # Get the success rate, i.e. p
failprob(d)    # Get the failure rate, i.e. 1 - p</code></pre><p>External links:</p><ul><li><a href="http://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution on Wikipedia</a></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Distributions.Poisson" href="#Distributions.Poisson"><code>Distributions.Poisson</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Poisson(λ)</code></pre><p>A <em>Poisson distribution</em> descibes the number of independent events occurring within a unit time interval, given the average rate of occurrence <code>λ</code>.</p><div>\[P(X = k) = \frac{\lambda^k}{k!} e^{-\lambda}, \quad \text{ for } k = 0,1,2,\ldots.\]</div><pre><code class="language-julia">Poisson()        # Poisson distribution with rate parameter 1
Poisson(lambda)       # Poisson distribution with rate parameter lambda

params(d)        # Get the parameters, i.e. (λ,)
mean(d)          # Get the mean arrival rate, i.e. λ</code></pre><p>External links:</p><ul><li><a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution on Wikipedia</a></li></ul></div></section></article><p>A <code>GeneralizedLinearMixedModel</code> object is generated from a formula, data frame and distribution family.</p><pre><code class="language-julia">julia&gt; verbagg = MixedModels.dataset(:verbagg)
7584×9 DataFrames.DataFrame. Omitted printing of 2 columns
│ Row  │ subj   │ item        │ anger │ gender │ btype  │ situ   │ mode   │
│      │ String │ String      │ Int8  │ String │ String │ String │ String │
├──────┼────────┼─────────────┼───────┼────────┼────────┼────────┼────────┤
│ 1    │ S001   │ S1WantCurse │ 20    │ M      │ curse  │ other  │ want   │
│ 2    │ S002   │ S1WantCurse │ 11    │ M      │ curse  │ other  │ want   │
│ 3    │ S003   │ S1WantCurse │ 17    │ F      │ curse  │ other  │ want   │
│ 4    │ S004   │ S1WantCurse │ 21    │ F      │ curse  │ other  │ want   │
│ 5    │ S005   │ S1WantCurse │ 17    │ F      │ curse  │ other  │ want   │
│ 6    │ S006   │ S1WantCurse │ 21    │ F      │ curse  │ other  │ want   │
│ 7    │ S007   │ S1WantCurse │ 39    │ F      │ curse  │ other  │ want   │
⋮
│ 7577 │ S309   │ S4DoShout   │ 14    │ F      │ shout  │ self   │ do     │
│ 7578 │ S310   │ S4DoShout   │ 23    │ F      │ shout  │ self   │ do     │
│ 7579 │ S311   │ S4DoShout   │ 24    │ M      │ shout  │ self   │ do     │
│ 7580 │ S312   │ S4DoShout   │ 17    │ M      │ shout  │ self   │ do     │
│ 7581 │ S313   │ S4DoShout   │ 20    │ F      │ shout  │ self   │ do     │
│ 7582 │ S314   │ S4DoShout   │ 25    │ F      │ shout  │ self   │ do     │
│ 7583 │ S315   │ S4DoShout   │ 23    │ F      │ shout  │ self   │ do     │
│ 7584 │ S316   │ S4DoShout   │ 12    │ F      │ shout  │ self   │ do     │

julia&gt; const vaform = @formula(r2 ~ 1 + anger + gender + btype + situ + (1|subj) + (1|item));

julia&gt; mdl = GeneralizedLinearMixedModel(vaform, verbagg, Bernoulli());

julia&gt; typeof(mdl)
GeneralizedLinearMixedModel{Float64}
</code></pre><p>A separate call to <code>fit!</code> can be used to fit the model. This involves optimizing an objective function, the Laplace approximation to the deviance, with respect to the parameters, which are <span>$\beta$</span>, the fixed-effects coefficients, and <span>$\theta$</span>, the covariance parameters. The starting estimate for <span>$\beta$</span> is determined by fitting a GLM to the fixed-effects part of the formula</p><pre><code class="language-julia">julia&gt; mdl.β
6-element Array{Float64,1}:
  0.20605302210322748 
  0.039940376051149876
  0.23131667674984463 
 -0.7941857249205364  
 -1.539188208545692   
 -0.7766556048305915  
</code></pre><p>and the starting estimate for <span>$\theta$</span>, which is a vector of the two standard deviations of the random effects, is chosen to be</p><pre><code class="language-julia">julia&gt; mdl.θ
2-element Array{Float64,1}:
 1.0
 1.0
</code></pre><p>The Laplace approximation to the deviance requires determining the conditional modes of the random effects. These are the values that maximize the conditional density of the random effects, given the model parameters and the data. This is done using Penalized Iteratively Reweighted Least Squares (PIRLS). In most cases PIRLS is fast and stable. It is simply a penalized version of the IRLS algorithm used in fitting GLMs.</p><p>The distinction between the &quot;fast&quot; and &quot;slow&quot; algorithms in the <code>MixedModels</code> package (<code>nAGQ=0</code> or <code>nAGQ=1</code> in <code>lme4</code>) is whether the fixed-effects parameters, <span>$\beta$</span>, are optimized in PIRLS or in the nonlinear optimizer. In a call to the <code>pirls!</code> function the first argument is a <code>GeneralizedLinearMixedModel</code>, which is modified during the function call. (By convention, the names of such <em>mutating functions</em> end in <code>!</code> as a warning to the user that they can modify an argument, usually the first argument.) The second and third arguments are optional logical values indicating if <span>$\beta$</span> is to be varied and if verbose output is to be printed.</p><pre><code class="language-julia">julia&gt; pirls!(mdl, true, true)
varyβ = true, obj₀ = 10210.853438905406, β = [0.20605302210322748, 0.039940376051149876, 0.23131667674984463, -0.7941857249205364, -1.539188208545692, -0.7766556048305915]
   1: 8301.483049027265
   2: 8205.604285133919
   3: 8201.89659746689
   4: 8201.848598910705
   5: 8201.848559060705
   6: 8201.848559060621
Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  r2 ~ 1 + anger + gender + btype + situ + (1 | subj) + (1 | item)
  Distribution: Bernoulli{Float64}
  Link: LogitLink()

  Deviance: 8201.8486

Variance components:
        Column    Variance  Std.Dev. 
subj (Intercept)  0.8920336 0.9444753
item (Intercept)  0.8920336 0.9444753

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
─────────────────────────────────────────────────────
                Estimate  Std.Error  z value  P(&gt;|z|)
─────────────────────────────────────────────────────
(Intercept)    0.218535    0.464651     0.47   0.6381
anger          0.0514385   0.012319     4.18   &lt;1e-4 
gender: M      0.290225    0.140555     2.06   0.0389
btype: scold  -0.979124    0.476395    -2.06   0.0399
btype: shout  -1.95402     0.477182    -4.09   &lt;1e-4 
situ: self    -0.979493    0.389283    -2.52   0.0119
─────────────────────────────────────────────────────
</code></pre><pre><code class="language-julia">julia&gt; deviance(mdl)
8201.848559060621
</code></pre><pre><code class="language-julia">julia&gt; mdl.β
6-element Array{Float64,1}:
  0.21853493716516456
  0.05143854258081136
  0.2902245416630128 
 -0.9791237061899594 
 -1.9540167628139584 
 -0.9794925718037184 
</code></pre><pre><code class="language-julia">julia&gt; mdl.θ # current values of the standard deviations of the random effects
2-element Array{Float64,1}:
 1.0
 1.0
</code></pre><p>If the optimization with respect to <span>$\beta$</span> is performed within PIRLS then the nonlinear optimization of the Laplace approximation to the deviance requires optimization with respect to <span>$\theta$</span> only. This is the &quot;fast&quot; algorithm. Given a value of <span>$\theta$</span>, PIRLS is used to determine the conditional estimate of <span>$\beta$</span> and the conditional mode of the random effects, <strong>b</strong>.</p><pre><code class="language-julia">julia&gt; mdl.b # conditional modes of b
2-element Array{Array{Float64,2},1}:
 [-0.6007716038488811 -1.9322680866219628 … -0.14455373975335703 -0.5752238433557011]
 [-0.1863641874790072 0.021422773585936136 … 0.6410383402097959 0.6496779078973146]  
</code></pre><pre><code class="language-julia">julia&gt; fit!(mdl, fast=true, verbose=true);
varyβ = true, obj₀ = 10251.003116042944, β = [0.21853493716516456, 0.05143854258081136, 0.2902245416630128, -0.9791237061899594, -1.9540167628139584, -0.9794925718037184]
   1: 8292.390783437771
   2: 8204.692089323944
   3: 8201.87681054392
   4: 8201.848569551963
   5: 8201.848559060627
   6: 8201.848559060621
f_1: 8201.848559060621 [1.0, 1.0]
varyβ = true, obj₀ = 10565.660003719015, β = [0.2185349371650122, 0.05143854258081871, 0.2902245416630129, -0.9791237061900514, -1.954016762814042, -0.9794925718036336]
   1: 8356.488185490085
   2: 8200.93270879382
   3: 8190.472561852921
   4: 8190.119918353261
   5: 8190.117817016412
   6: 8190.117816802471
f_2: 8190.117816802471 [1.75, 1.0]
varyβ = true, obj₀ = 10317.246304689335, β = [0.193420587351331, 0.05738776368387292, 0.31799603952605654, -1.0485464263733457, -2.0960319812114325, -1.051128801683704]
   1: 8322.599130522533
   2: 8227.32605948889
   3: 8224.479329416503
   4: 8224.450989053388
   5: 8224.450978980774
   6: 8224.45097898077
f_3: 8224.45097898077 [1.0, 1.75]
varyβ = true, obj₀ = 9776.196238005325, β = [0.21876438895192785, 0.05148884100230967, 0.29046070478860453, -0.9797643406723213, -1.9572962794109496, -0.9812079030634242]
   1: 9035.725240344309
   2: 9026.060558389583
   3: 9026.003998095694
   4: 9026.003906404212
   5: 9026.003906403306
f_4: 9026.003906403306 [0.25, 1.0]
varyβ = true, obj₀ = 10149.608772216947, β = [0.19893695106098339, 0.04180670725991061, 0.24189864813053638, -0.8153959340434729, -1.6334718386669707, -0.8201772064234891]
   1: 8286.346044727728
   2: 8208.252412394584
   3: 8205.812828017322
   4: 8205.793778239711
   5: 8205.793775487933
f_5: 8205.793775487933 [1.0, 0.25]
varyβ = true, obj₀ = 10406.590456413769, β = [0.21728094120287472, 0.05073109226840579, 0.28679620590416827, -0.9699881548970221, -1.9127335969466084, -0.959182389479004]
   1: 8290.678720764578
   2: 8163.613865769224
   3: 8157.16743328995
   4: 8157.041222813376
   5: 8157.041032394058
   6: 8157.041032392805
f_6: 8157.041032392805 [1.3858293823679781, 0.7364567143287959]
varyβ = true, obj₀ = 10334.28560906147, β = [0.20721523412593235, 0.05490998576682092, 0.30652969414239317, -1.022780670066828, -2.040115543154571, -1.0227350541867712]
   1: 8461.553656168611
   2: 8371.201859251129
   3: 8367.771392346047
   4: 8367.724263488171
   5: 8367.724223100673
   6: 8367.724223100575
f_7: 8367.724223100575 [1.3371524465545712, 0.0]
varyβ = true, obj₀ = 10441.99288890889, β = [0.21310775427579984, 0.05262369623725068, 0.2959485161926351, -0.9987562912090376, -1.933937463915696, -0.973535338019592]
   1: 8308.813446103564
   2: 8177.318105403589
   3: 8170.433438731546
   4: 8170.289084465012
   5: 8170.288828193686
   6: 8170.28882819152
f_8: 8170.28882819152 [1.4136494266702293, 1.1104233507216976]
varyβ = true, obj₀ = 10394.123754834078, β = [0.20658453714127423, 0.0552133731832723, 0.3079133180419622, -1.0262965361212382, -2.0504566216498024, -1.0280362485644596]
   1: 8283.540345224603
   2: 8163.918696581624
   3: 8158.905795827854
   4: 8158.829375849116
   5: 8158.829317592089
   6: 8158.829317591996
f_9: 8158.829317591996 [1.2722464441311803, 0.7628110428959164]
varyβ = true, obj₀ = 10449.122512319118, β = [0.21122282489398722, 0.05400548405721326, 0.30230959849208233, -1.012299568744952, -2.0190655839379748, -1.0121221456081502]
   1: 8299.982309744815
   2: 8168.539378419906
   3: 8162.061112545691
   4: 8161.933600781367
   5: 8161.933408558587
   6: 8161.933408557297
f_10: 8161.933408557297 [1.4093623667537467, 0.8680844411698708]
varyβ = true, obj₀ = 10414.107933396312, β = [0.20653729580009558, 0.05513368643868377, 0.30755767449650495, -1.0253459434061065, -2.0468884488247587, -1.026189553447334]
   1: 8286.13831666331
   2: 8161.902045658137
   3: 8156.39292594619
   4: 8156.301068837268
   5: 8156.3009800186865
   6: 8156.300980018444
f_11: 8156.300980018444 [1.3269393938015477, 0.7210153433277118]
varyβ = true, obj₀ = 10407.145905747264, β = [0.2092936301084143, 0.05444078616024455, 0.3043450395272343, -1.017410038992204, -2.0289135444050923, -1.017076254901679]
   1: 8284.877807447845
   2: 8161.708700091352
   3: 8156.20852356967
   4: 8156.116764659554
   5: 8156.116675394207
   6: 8156.116675393962
f_12: 8156.116675393962 [1.323649391663659, 0.7142754704852401]
varyβ = true, obj₀ = 10404.356312407224, β = [0.2093966506704068, 0.05441123183582412, 0.30420785069726425, -1.0170635346400367, -2.028105926151978, -1.016666961042806]
   1: 8284.155879798216
   2: 8161.539014566784
   3: 8156.092168783244
   4: 8156.002155450721
   5: 8156.002070048331
   6: 8156.002070048108
f_13: 8156.002070048108 [1.3184653242985764, 0.7088555585344528]
varyβ = true, obj₀ = 10404.53296455962, β = [0.20956728950797043, 0.05436677355362948, 0.30400097123018865, -1.0165433887574622, -2.0269623799216747, -1.0160886153662525]
   1: 8284.027704597474
   2: 8161.308933576594
   3: 8155.844273740122
   4: 8155.753676771538
   5: 8155.753590029703
   6: 8155.753590029471
f_14: 8155.753590029471 [1.320716938857125, 0.7017015224792088]
varyβ = true, obj₀ = 10406.080590126865, β = [0.20947609033470668, 0.054381708709045844, 0.30407138125848776, -1.0167138677795922, -2.0271985835155646, -1.0162057468818588]
   1: 8283.987555395457
   2: 8160.874305438911
   3: 8155.367283709277
   4: 8155.275310500366
   5: 8155.275220625106
   6: 8155.275220624853
f_15: 8155.275220624853 [1.3263558167617484, 0.6878017722664622]
varyβ = true, obj₀ = 10409.588006838827, β = [0.20925331974756306, 0.05442052407950844, 0.3042539578337623, -1.017157811376714, -2.0278787178031767, -1.016544913381582]
   1: 8284.07087504108
   2: 8160.104155117678
   3: 8154.505123130811
   4: 8154.410093357725
   5: 8154.4099961590755
   6: 8154.409996158768
f_16: 8154.409996158768 [1.3385857165559656, 0.6604078030209621]
varyβ = true, obj₀ = 10422.176224405805, β = [0.20876798544822775, 0.054504010629243094, 0.30464664470266795, -1.0181072531046664, -2.0293349027599, -1.0172718754793673]
   1: 8286.14893056937
   2: 8159.408927883488
   3: 8153.50100477619
   4: 8153.395082917572
   5: 8153.394956151762
   6: 8153.394956151182
f_17: 8153.394956151182 [1.3758192930036834, 0.6133582463289808]
varyβ = true, obj₀ = 10427.671854015935, β = [0.20732305195122236, 0.05476609730563587, 0.3058757219213609, -1.021062848044387, -2.0344083911048347, -1.019819941218364]
   1: 8286.691066751966
   2: 8158.86955655105
   3: 8152.851268971587
   4: 8152.741083892389
   5: 8152.740942646411
   6: 8152.74094264564
f_18: 8152.74094264564 [1.395150899360877, 0.5630963988303244]
varyβ = true, obj₀ = 10412.134716835524, β = [0.2064753179535223, 0.0548743832097538, 0.30638967606907763, -1.0222230837236579, -2.035530371922711, -1.0203755723552477]
   1: 8282.017684913284
   2: 8157.522016815873
   3: 8151.862297991567
   4: 8151.764833400504
   5: 8151.764726543848
   6: 8151.764726543412
f_19: 8151.764726543412 [1.3676323395322791, 0.5091235621211307]
varyβ = true, obj₀ = 10364.111639274717, β = [0.20730625718468784, 0.05461186270261239, 0.30517481958643167, -1.0191804210360513, -2.0276807789667903, -1.0164075726140147]
   1: 8271.970907129926
   2: 8157.55555775327
   3: 8152.875645706794
   4: 8152.808979106626
   5: 8152.808935981371
   6: 8152.808935981312
f_20: 8152.808935981312 [1.2677640577723635, 0.47512314631845676]
varyβ = true, obj₀ = 10420.04918502711, β = [0.2106775805667401, 0.05376804338834322, 0.3012295551223543, -1.0092843628508967, -2.006396181496647, -1.005678743638404]
   1: 8285.75225336104
   2: 8159.073978096158
   3: 8152.980811897899
   4: 8152.8665773069415
   5: 8152.8664150019495
   6: 8152.866415000816
f_21: 8152.866415000816 [1.4147967067161125, 0.4710990870343405]
varyβ = true, obj₀ = 10395.489389328499, β = [0.20541659536892584, 0.05492082437718557, 0.30662779729828926, -1.0225711061467748, -2.0331407851432215, -1.0191682418229173]
   1: 8278.256525896244
   2: 8157.1546726389915
   3: 8151.854900401958
   4: 8151.769667985616
   5: 8151.769591419707
   6: 8151.769591419504
f_22: 8151.769591419504 [1.3258860654164732, 0.5275226916672942]
varyβ = true, obj₀ = 10406.847022583572, β = [0.20886367217928128, 0.0543031785424332, 0.3037280282635405, -1.0156611090060534, -2.0211403904584073, -1.0131090903120958]
   1: 8281.19520217958
   2: 8157.492470480421
   3: 8151.835428349385
   4: 8151.737863139839
   5: 8151.737755041621
   6: 8151.737755041168
f_23: 8151.737755041168 [1.3668062704800998, 0.4986062131355091]
varyβ = true, obj₀ = 10396.623183200287, β = [0.2072982541958861, 0.05459363004474792, 0.3050918349452312, -1.0189553661917297, -2.0268543124626426, -1.0159907029314292]
   1: 8278.460002110576
   2: 8157.05332393766
   3: 8151.673309975076
   4: 8151.585246119001
   5: 8151.585161942598
   6: 8151.585161942338
f_24: 8151.585161942338 [1.3397371137806189, 0.4934921982211279]
varyβ = true, obj₀ = 10393.609316251614, β = [0.20825496597645685, 0.054376602982546166, 0.3040778501929814, -1.016462598792412, -2.0216118170023134, -1.0133460788423416]
   1: 8277.918179767978
   2: 8157.043669989391
   3: 8151.689440593426
   4: 8151.602147764972
   5: 8151.602064961408
   6: 8151.602064961155
f_25: 8151.602064961155 [1.3375752795410298, 0.48631052204348374]
varyβ = true, obj₀ = 10397.764486561106, β = [0.20830542908593824, 0.054350965531124264, 0.3039593106175127, -1.0161551809708145, -2.0207219752080294, -1.0128978657453545]
   1: 8278.935236560126
   2: 8157.1452308191
   3: 8151.691194952197
   4: 8151.600591190689
   5: 8151.600500691274
   6: 8151.600500690965
f_26: 8151.600500690965 [1.3469240905715114, 0.49134805162931744]
varyβ = true, obj₀ = 10395.774178198204, β = [0.2079898825144615, 0.054430597969304946, 0.30433091270636803, -1.017080341971835, -2.0227854258226756, -1.0139382184732906]
   1: 8278.406326249316
   2: 8157.064425497867
   3: 8151.671951343675
   4: 8151.583465843252
   5: 8151.583380717897
   6: 8151.583380717631
f_27: 8151.583380717631 [1.3395817610366942, 0.4973372768677059]
varyβ = true, obj₀ = 10395.443943189708, β = [0.20827453095430626, 0.054379842471567345, 0.3040922464550034, -1.0165060438642397, -2.0218392967294077, -1.0134606162647075]
   1: 8278.362655736963
   2: 8157.064544983578
   3: 8151.672160260854
   4: 8151.58367501763
   5: 8151.583589892587
   6: 8151.5835898923215
f_28: 8151.5835898923215 [1.3392718767167426, 0.49802026420891227]
varyβ = true, obj₀ = 10395.535382999728, β = [0.20828805461944533, 0.054378180211740394, 0.3040843349543691, -1.0164879468260724, -2.021827115712866, -1.0134544452465744]
   1: 8278.380710340956
   2: 8157.066651045131
   3: 8151.671999799075
   4: 8151.58343280049
   5: 8151.583347442415
   6: 8151.583347442146
f_29: 8151.583347442146 [1.3397210818326308, 0.4969553853429551]
varyβ = true, obj₀ = 10395.855932031242, β = [0.2082681763394181, 0.0543805024243515, 0.3040954094451232, -1.0165130522338501, -2.021839767590585, -1.0134608700296037]
   1: 8278.456305937356
   2: 8157.074106468341
   3: 8151.672400464069
   4: 8151.583600201323
   5: 8151.583514305466
   6: 8151.583514305195
f_30: 8151.583514305195 [1.3404085881860572, 0.49725511024015007]
varyβ = true, obj₀ = 10395.450813219028, β = [0.2082447332800359, 0.05438627252764157, 0.3041223466790911, -1.0165800287331164, -2.0219863550428467, -1.0135347946044646]
   1: 8278.35336160107
   2: 8157.063550036686
   3: 8151.67195413513
   4: 8151.583485621319
   5: 8151.583400470982
   6: 8151.583400470716
f_31: 8151.583400470716 [1.33957287723201, 0.49622017421929415]
varyβ = true, obj₀ = 10395.72665354137, β = [0.20827079143716004, 0.054378483644552535, 0.30408610909570555, -1.0164886208436281, -2.0217637169707507, -1.0134225446121246]
   1: 8278.426177247826
   2: 8157.071791450592
   3: 8151.672300242834
   4: 8151.583568754749
   5: 8151.583482990037
   6: 8151.583482989767
f_32: 8151.583482989767 [1.3403128971105505, 0.49649467428728034]
varyβ = true, obj₀ = 10395.480743443582, β = [0.20824538649009727, 0.05438464054172478, 0.3041148615287878, -1.0165600218528488, -2.021918394381872, -1.0135005493945886]
   1: 8278.364600272647
   2: 8157.064659600587
   3: 8151.671928507907
   4: 8151.583425349218
   5: 8151.583340139287
   6: 8151.583340139021
f_33: 8151.583340139021 [1.339558330468156, 0.4968332751141721]
varyβ = true, obj₀ = 10395.442267529454, β = [0.20827353831174117, 0.05437907696035404, 0.30408876413380453, -1.016496426971929, -2.0218016612248904, -1.0134416550286387]
   1: 8278.359702896048
   2: 8157.064704875592
   3: 8151.671931416343
   4: 8151.5834264301775
   5: 8151.583341215452
   6: 8151.583341215184
f_34: 8151.583341215184 [1.3395286260114363, 0.4969021419803052]
varyβ = true, obj₀ = 10395.479774747564, β = [0.20827484776302754, 0.05437892187902116, 0.3040880249694642, -1.016494747325167, -2.0218007362740926, -1.013441185462387]
   1: 8278.368369188824
   2: 8157.06558474131
   3: 8151.671959910015
   4: 8151.583426365366
   5: 8151.583341082107
   6: 8151.583341081839
f_35: 8151.583341081839 [1.3396255162902735, 0.4968666077604276]
varyβ = true, obj₀ = 10395.478844624251, β = [0.20827126293066459, 0.05437964575018255, 0.30409141879336904, -1.0165030367584567, -2.021816263524065, -1.0134490187638063]
   1: 8278.367429147173
   2: 8157.065445657684
   3: 8151.671955750738
   4: 8151.583426429476
   5: 8151.583341154114
   6: 8151.583341153844
f_36: 8151.583341153844 [1.339626756923135, 0.4968025697358324]
varyβ = true, obj₀ = 10395.453884488728, β = [0.20827098617875134, 0.05437958168696216, 0.30409113168519963, -1.016502197472787, -2.021812254819943, -1.0134469996597328]
   1: 8278.36183706667
   2: 8157.064895679179
   3: 8151.671936855659
   4: 8151.583425364515
   5: 8151.583340132133
   6: 8151.583340131869
f_37: 8151.583340131869 [1.3395639000172377, 0.49683278387720115]
varyβ = true, obj₀ = 10395.451247339135, β = [0.20827333787757413, 0.05437912035977906, 0.3040889672864563, -1.016496926543131, -2.0218026564220364, -1.0134421570144092]
   1: 8278.361538695954
   2: 8157.0649121919505
   3: 8151.67193745469
   4: 8151.583425366178
   5: 8151.583340132133
   6: 8151.583340131867
</code></pre><p>The optimization process is summarized by</p><pre><code class="language-julia">julia&gt; mdl.LMM.optsum
Initial parameter vector: [1.0, 1.0]
Initial objective value:  8201.848559060621

Optimizer (from NLopt):   LN_BOBYQA
Lower bounds:             [0.0, 0.0]
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10]
initial_step:             [0.75, 0.75]
maxfeval:                 -1

Function evaluations:     37
Final parameter vector:   [1.3395639000172377, 0.49683278387720115]
Final objective value:    8151.583340131869
Return code:              FTOL_REACHED

</code></pre><p>As one would hope, given the name of the option, this fit is comparatively fast.</p><pre><code class="language-julia">julia&gt; @time(fit!(GeneralizedLinearMixedModel(vaform,
    verbagg, Bernoulli()), fast=true))
  0.336994 seconds (2.12 M allocations: 24.802 MiB)
Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  r2 ~ 1 + anger + gender + btype + situ + (1 | subj) + (1 | item)
  Distribution: Bernoulli{Float64}
  Link: LogitLink()

  Deviance: 8151.5833

Variance components:
        Column    Variance   Std.Dev.  
subj (Intercept)  1.63965872 1.28049159
item (Intercept)  0.22555221 0.47492337

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
─────────────────────────────────────────────────────
                Estimate  Std.Error  z value  P(&gt;|z|)
─────────────────────────────────────────────────────
(Intercept)    0.208273   0.387547      0.54   0.5910
anger          0.0543791  0.0160145     3.40   0.0007
gender: M      0.304089   0.182791      1.66   0.0962
btype: scold  -1.0165     0.246175     -4.13   &lt;1e-4 
btype: shout  -2.0218     0.247803     -8.16   &lt;1e-15
situ: self    -1.01344    0.201588     -5.03   &lt;1e-6 
─────────────────────────────────────────────────────
</code></pre><p>The alternative algorithm is to use PIRLS to find the conditional mode of the random effects, given <span>$\beta$</span> and <span>$\theta$</span> and then use the general nonlinear optimizer to fit with respect to both <span>$\beta$</span> and <span>$\theta$</span>. Because it is slower to incorporate the <span>$\beta$</span> parameters in the general nonlinear optimization, the fast fit is performed first and used to determine starting estimates for the more general optimization.</p><pre><code class="language-julia">julia&gt; @time mdl1 = fit(MixedModel, vaform, verbagg, Bernoulli())
  1.712681 seconds (10.26 M allocations: 95.830 MiB, 1.64% gc time)
Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  r2 ~ 1 + anger + gender + btype + situ + (1 | subj) + (1 | item)
  Distribution: Bernoulli{Float64}
  Link: LogitLink()

  Deviance: 8151.3998

Variance components:
        Column    Variance   Std.Dev.  
subj (Intercept)  1.64374179 1.28208494
item (Intercept)  0.22468305 0.47400743

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
────────────────────────────────────────────────────
               Estimate  Std.Error  z value  P(&gt;|z|)
────────────────────────────────────────────────────
(Intercept)    0.196478  0.387752      0.51   0.6124
anger          0.05752   0.0160365     3.59   0.0003
gender: M      0.320955  0.183031      1.75   0.0795
btype: scold  -1.05815   0.245763     -4.31   &lt;1e-4 
btype: shout  -2.10482   0.247412     -8.51   &lt;1e-16
situ: self    -1.05521   0.201259     -5.24   &lt;1e-6 
────────────────────────────────────────────────────
</code></pre><p>This fit provided slightly better results (Laplace approximation to the deviance of 8151.400 versus 8151.583) but took 6 times as long. That is not terribly important when the times involved are a few seconds but can be important when the fit requires many hours or days of computing time.</p><p>The comparison of the slow and fast fit is available in the optimization summary after the slow fit.</p><pre><code class="language-julia">julia&gt; mdl1.LMM.optsum
Initial parameter vector: [0.20605302210322748, 0.039940376051149876, 0.23131667674984463, -0.7941857249205364, -1.539188208545692, -0.7766556048305915, 1.0, 1.0]
Initial objective value:  8204.421187737946

Optimizer (from NLopt):   LN_BOBYQA
Lower bounds:             [-Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, 0.0]
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10]
initial_step:             [0.20605302210322748, 0.039940376051149876, 0.23131667674984463, -0.7941857249205364, -1.539188208545692, -0.7766556048305915, 0.75, 0.75]
maxfeval:                 -1

Function evaluations:     186
Final parameter vector:   [0.19647816858060044, 0.05751998568928086, 0.32095543494087175, -1.0581469196007018, -2.1048216065649847, -1.0552100825254875, 1.3397449401710144, 0.49532526439691343]
Final objective value:    8151.399761446788
Return code:              FTOL_REACHED

</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../constructors/">« Model constructors</a><a class="docs-footer-nextpage" href="../GaussHermite/">Normalized Gauss-Hermite Quadrature »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 16 February 2020 16:02">Sunday 16 February 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
